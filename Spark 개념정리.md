# Spark 개념정리

## 아파치 스파크란

- 아파치 스파크는 통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합
- 가장 활발하게 개발되고 있는 병렬 처리 오픈소스 엔진이며 표준도구가 되어가고 있다.
- 파이썬, 자바, 스칼라, R을 지원하며 SQL, 스트리밍, 머신러닝까지 넓은 범위의 라이브러리를 제공한다.
- 단일 노트북 환경에서부터 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행할 수 있다.



## 아파치 스파크의 철학

'빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합' 이라는 문장을 기준으로 파헤쳐보자



### 통합

- 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝, 스트림 처리에 이르기까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계되어 있다.

- 스파크의 이러한 개발 사상은 현실세계의 데이터 분석 작업 (주피터 노트북과 같은 대화형 분석도구를 사용하거나 운영용 애플리케이션 개발과 같은 전통적인 소프트웨어 개발 작업으로) 이 다양한 처리 유형과 라이브러리를 결합해 수행된다는 통찰에서 비롯되었다.

스파크의 통합 특성을 활용하면 기존의 데이터 분석 작업을 더 쉽고 효율적으로 수행할 수 있다.

- 스파크는 일관성 있는 조합형 API를 제공하므로 작은 코드 조각이나 기존의 라이브러리를 사용해 애플리케이션을 만들 수 있다.
- 조합형 API 만으로는 문제를 해결할 수 없을 때는 직접 스파크 기반의 라이브러리를 만들 수도 있다.
- 스파크의 API는 사용자 애플리케이션에서 다른 라이브러리의 기능을 조합해 더 나은 성능을 발휘할 수 있도록 설계되었다.
  예를들면 SQL 쿼리로 데이터를 읽고 ML 라이브러리로 머신러닝 모델을 평가해야할 경우 스파크 엔진은 이 두 단계를 하나로 병합하고 데이터를 한 번만 조회할 수 있게 해준다.
- 스파크의 범용 API와 처리능력을 활용하면 대화형 분석과 운영용 애플리케이션에 필요한 플랫폼을 얻을 수 있다.



### 컴퓨팅 엔진

- 스파크는 데이터를 연산하는 역할만 수행할 뿐 영구 저장소의 역할은 수행하지 않는다.
- 스파크는 데이터 저장 위치에 상관 없이 처리에 집중한다.
- 서로 다른 저장소 시스템을 매우 유사하게 볼 수 있도록 만들어졌다.

#### 하둡과의 비교

하둡의 경우 범용 서버 클러스터 환경에서 저비용 저장 장치를 사용하도록 설계된 하둡 파일 시스템과 컴퓨팅 시스템(맵리듀스)를 가지고 있으며 두 시스템은 매우 밀접하게 연관되어 있다. 하둡과 같은 구조에서는 하나의 시스템만 단독으로 사용하기 어렵다.



### 라이브러리

스파크는 엔진에서 제공하는 표준 라이브러리와 오픈소스 커뮤티니에서 서드파티 패키지 형태로 제공하는 다양한 외부라이브러리를 지원한다. 스파크의 표준 라이브러리는 여러 오픈소스 프로젝트의 집합체이다.

#### 스파크가 제공하는 라이브러리

- SQL과 구조화된 데이터를 제공하는 스파크 SQL
- 머신러닝을 지원하는 MLlib
- 스트림 처리 기능을 제공하는 스파크 스트리밍, 구조적 스트리밍
- 그래프 분석 엔진인 GraphX 라이브러리

#### 외부 라이브러리

- 다양한 저장소 시스템을 위한 커넥터
- 머신러닝을 위한 알고리즘 들
- http://spark-packages.org (외부라이브러리 목록)



## 스파크의 등장 배경

- 물리적인 방열 한계로 인해 단일 프로세서의 성능 향상은 2005년경에 멈춤
- 모든 코어가 같은 속도로 동작하는 병렬 CPU 코어를 더 많이 추가하는 방향으로 선회
- 1TB의 데이터를 저장하는데 드는 비용은 대략 14개월마다 절반으로 줄고 있음
- 데이터 수집을 위한 기술 비용은 계속 저렴해지고 정밀도는 개선되고 있음
- 결론적으로 데이터 수집비용이 저렴해지면서 그 양이 거대해졌다
- 그러나 지난 50년간 개발된 소프트웨어는 단일 프로세서의 성능향상에 자신의 성능향상을 맡겼기 때문에 더이상 성능향상이 어렵다. 전통적인 프로그래밍 모델도 마찬가지이다.
- 따라서 새로운 프로그래밍 모델이 필요해졌다.



### 신경망 학습에서 spark의 역할

#### 하이퍼파라미터 튜닝

교육시간을 단축하고 오류율을 줄인다. 

tensorflow의 하이퍼파라미터 튜닝이 tensorflow 자체를 분산하여 배포하지 않아도 내부에서 병렬적으로 이루어진다. 따라서 이 과정을 spark를 사용하여 분산하여 해결한다 (데이터 및 모델 description을 브로드캐스팅하고 fault-tolerance하게 클러스터를 구축할 수 있다는 의미).



#### 대규모 모델 배포

Spark를 사용하여 많은 양의 데이터에 훈련된 모델을 만들 수 있다.



### SparkFlow : Train Tensorflow Models with Apache Spark Pipelines

교수님께서 주신 링크 2번 내용.

SparkFlow는 Tensorflow on spark 를 사용하기 위한 것이다. Spark에서 Tensorflow를 사용할 때 간단하고 이해하기 쉬운 인터페이스를 제공한다. 

Spark와 SparkML은 대규모 데이터 세트에서 복잡한 파이프라인을 조율할 수 있는 아키텍쳐를 지원한다.
하지만 현재 이미지용 CNN이나 자연어 처리를 위한 RNN등은 지원하지 않는다.

feature engineering pipeline은 원시데이터를 정리하고 학스모델로 변환 및 준비하는 프로세스를 크게 단순화시킨다.

Tensorflow는 다양한 신경망을 학습시키는데 적합하지만 Spark에 비해 대형 데이터셋의 파이프라인에 대한 기능지원이 부족하다. 

Spark에서 Tensorflow 모델을 교육하기 위해 다른 오픈소스 라이브러리가 존재하지만, SparkML의 기능을 모두 이용하는 개발자는 거의 없다.

이에따라 2018에 LifeOmic에서 Sparkflow를 출시했다. Spark의 파이프라인 API 인터페이스를 Tensorflow와 결합한다. pip을 통해 설치가 가능하다.  



## 스파크의 기본 아키텍쳐

![스파크 기능 구성](.\images\스파크 기능 구성.jpg)

![스파크 애플리케이션 아키텍쳐](.\images\스파크 애플리케이션 아키텍쳐.PNG)

- 클러스터 모드일 경우 : 드라이버와 익스큐터는 프로세스이므로 같은 머신 혹은 다른 머신에서 실행할 수 있다.
- 로컬 모드일 경우 : 드라이버와 익스큐터를 스레드의 형태로 실행한다.



스파크 애플리케이션은 드라이버 프로세스와 다수의 익스큐터 프로세스로 구성된다.

1. 드라이버 프로세스 : 클러스터 노드 중 하나에서 실행되며 main() 함수를 실행한다. 스파크 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 스케줄링 역할을 수행한다.
2. 익스큐터 : 드라이버 프로세스가 할당한 작업을 수행한다. 할당받은 코드를 실행하고 진행 상황을 드라이버 노드에 보고한다.

3. 클러스터 매니저 : 클러스터 매니저는 모든 스파크 애플리케이션과 관련된 프로세스를 유지하는 역할을 한다. 스파크 애플리케이션을 실행할 클러스터 머신을 유지한다. '드라이버'와 '워커' 라는 개념을 가지고 있으며 프로세스가 아닌 물리적인 머신에 연결되는 개념이다. (아직 완전히 이해하지 못함)

### SparkSession

스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스로 제어한다.
SparkSession 인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행한다. 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응한다. 스칼라나 파이썬 콘솔을 사용하면 spark변수로 SparkSession을 사용할 수 있다. (tensorflow에서의 Session과 비슷한가?)

### 파티션

파티션은 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미한다.
DataFrame의 파티션은 실행중에 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타낸다.

파티션이 하나라면 익스큐터가 몇개든 병렬성 1
반대로 익스큐터가 1개뿐이라면 파티션이 몇개든 병렬성 1

DataFrame을 이용하면 파티션을 수동으로 혹은 개별적으로 처리할 필요없이 물리적 파티션에 데이터 변환용 함수만 지정하면 스파크가 실제 처리 방법을 결정한다.

### 트랜스포메이션

파이썬에서 DataFrame에서 짝수를 찾는 예제:
divisBy2 = myRange.where("number % 2 = 0")

1개의 파티션이 1개의 출력 파티션에만 영향을 미치면 좁은 트랜스포메이션
1개의 파티션이 여러 출력 파티션에 영향을 미치면 넓은 트랜스포메이션

### 액션

트랜스포메이션을 사용해 논리적 실행 계획을 세웠다면 액션으로 계산을 실행시킨다.
바로 위의 예제코드를 실행시키는 예제:
divisBy2.count()



## 스파크의 분산 데이터 모음

### DataFrame

가장 대표적인 구조적 API.
DataFrame은 테이블의 데이터를 row와 column으로 간결하게 표현한다.
컬럼과 컬럼의 타입을 정의한 목록을 스키마라고 부른다.

DataFrame은 컬럼에 이름을 붙인 스프레드시트와 비슷하다고 생각할 수 있지만 차이점은 스파크의 DataFrame은 데이터센터의 서버에 분산되어 있다는 점이다.



### Dataset

Dataset은 자바와 스칼라의 정적 데이터 타입 코드를 지원하기 위해 고안된 스파크의 구조적 API 이다.
Dataset은 타입 안정성을 지원하며 동적 타입 언어인 파이썬과 R에서는 사용할 수 없습니다.

#### DataFrame과의 비교

DataFrame은 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row타입의 객체로 구성된 분산 컬렉션.

Dataset API는 DataFrame의 레코드를 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 객체 등의 고정 타입형 컬레션으로 다룰 수 있는 기능을 제공한다.



### RDD

RDD는 저수준 API이다.

RDD의 모든 레코드는 자바나 파이썬의 객체이므로 완벽하게 제어할 수 있다.
이러한 객체에는 사용자가 원하는 토맷을 사용해 원하는 모든 데이터를 저장할 수 있다.



## 참고자료

- Spark : The Definitive Guide , 스파크 완벽 가이드 , 한빛미디어
- http://www.itworld.co.kr/news/107192
- https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html - 교수님께서 주신 링크 1 (2016.01.25)
- https://medium.com/lifeomic/sparkflow-train-tensorflow-models-with-apache-spark-pipelines-74dca32f60f3 - - 교수님께서 주신 링크 2 (2018.08.24)
- https://github.com/lifeomic/sparkflow - 교수님께서 주신 링크 2에서 들어감
- [http://www.engear.net/wp/hyper-paramertesr-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/](http://www.engear.net/wp/hyper-paramertesr-머신러닝/) - 하이퍼파라미터
- https://databuzz-team.github.io/2018/12/05/hyperparameter-setting/ - 하이퍼파라미터 튜닝이란
- http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html - 베이지안 최적화 1
- https://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html - 베이지안 최적화 2
- https://gmlwjd9405.github.io/2018/09/14/process-vs-thread.html - 프로세스와 스레드의 차이




