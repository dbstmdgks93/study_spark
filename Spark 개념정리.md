# Spark 개념정리

## 아파치 스파크란

- 아파치 스파크는 통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합
- 가장 활발하게 개발되고 있는 병렬 처리 오픈소스 엔진이며 표준도구가 되어가고 있다.
- 파이썬, 자바, 스칼라, R을 지원하며 SQL, 스트리밍, 머신러닝까지 넓은 범위의 라이브러리를 제공한다.
- 단일 노트북 환경에서부터 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행할 수 있다.



## 아파치 스파크의 철학

'빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합' 이라는 문장을 기준으로 파헤쳐보자



### 통합

- 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝, 스트림 처리에 이르기까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계되어 있다.

- 스파크의 이러한 개발 사상은 현실세계의 데이터 분석 작업 (주피터 노트북과 같은 대화형 분석도구를 사용하거나 운영용 애플리케이션 개발과 같은 전통적인 소프트웨어 개발 작업으로) 이 다양한 처리 유형과 라이브러리를 결합해 수행된다는 통찰에서 비롯되었다.

스파크의 통합 특성을 활용하면 기존의 데이터 분석 작업을 더 쉽고 효율적으로 수행할 수 있다.

- 스파크는 일관성 있는 조합형 API를 제공하므로 작은 코드 조각이나 기존의 라이브러리를 사용해 애플리케이션을 만들 수 있다.
- 조합형 API 만으로는 문제를 해결할 수 없을 때는 직접 스파크 기반의 라이브러리를 만들 수도 있다.
- 스파크의 API는 사용자 애플리케이션에서 다른 라이브러리의 기능을 조합해 더 나은 성능을 발휘할 수 있도록 설계되었다.
  예를들면 SQL 쿼리로 데이터를 읽고 ML 라이브러리로 머신러닝 모델을 평가해야할 경우 스파크 엔진은 이 두 단계를 하나로 병합하고 데이터를 한 번만 조회할 수 있게 해준다.
- 스파크의 범용 API와 처리능력을 활용하면 대화형 분석과 운영용 애플리케이션에 필요한 플랫폼을 얻을 수 있다.



### 컴퓨팅 엔진

- 스파크는 데이터를 연산하는 역할만 수행할 뿐 영구 저장소의 역할은 수행하지 않는다.
- 스파크는 데이터 저장 위치에 상관 없이 처리에 집중한다.
- 서로 다른 저장소 시스템을 매우 유사하게 볼 수 있도록 만들어졌다.

#### 하둡과의 비교

하둡의 경우 범용 서버 클러스터 환경에서 저비용 저장 장치를 사용하도록 설계된 하둡 파일 시스템과 컴퓨팅 시스템(맵리듀스)를 가지고 있으며 두 시스템은 매우 밀접하게 연관되어 있다. 하둡과 같은 구조에서는 하나의 시스템만 단독으로 사용하기 어렵다.



### 라이브러리

스파크는 엔진에서 제공하는 표준 라이브러리와 오픈소스 커뮤티니에서 서드파티 패키지 형태로 제공하는 다양한 외부라이브러리를 지원한다. 스파크의 표준 라이브러리는 여러 오픈소스 프로젝트의 집합체이다.

#### 스파크가 제공하는 라이브러리

- SQL과 구조화된 데이터를 제공하는 스파크 SQL
- 머신러닝을 지원하는 MLlib
- 스트림 처리 기능을 제공하는 스파크 스트리밍, 구조적 스트리밍
- 그래프 분석 엔진인 GraphX 라이브러리

#### 외부 라이브러리

- 다양한 저장소 시스템을 위한 커넥터
- 머신러닝을 위한 알고리즘 들
- http://spark-packages.org (외부라이브러리 목록)



## 스파크의 등장 배경

- 물리적인 방열 한계로 인해 단일 프로세서의 성능 향상은 2005년경에 멈춤
- 모든 코어가 같은 속도로 동작하는 병렬 CPU 코어를 더 많이 추가하는 방향으로 선회
- 1TB의 데이터를 저장하는데 드는 비용은 대략 14개월마다 절반으로 줄고 있음
- 데이터 수집을 위한 기술 비용은 계속 저렴해지고 정밀도는 개선되고 있음
- 결론적으로 데이터 수집비용이 저렴해지면서 그 양이 거대해졌다
- 그러나 지난 50년간 개발된 소프트웨어는 단일 프로세서의 성능향상에 자신의 성능향상을 맡겼기 때문에 더이상 성능향상이 어렵다. 전통적인 프로그래밍 모델도 마찬가지이다.
- 따라서 새로운 프로그래밍 모델이 필요해졌다.



### 신경망 학습에서 spark의 역할

#### 하이퍼파라미터 튜닝

교육시간을 단축하고 오류율을 줄인다. 

tensorflow의 하이퍼파라미터 튜닝이 tensorflow 자체를 분산하여 배포하지 않아도 내부에서 병렬적으로 이루어진다. 따라서 이 과정을 spark를 사용하여 분산하여 해결한다 (데이터 및 모델 description을 브로드캐스팅하고 fault-tolerance하게 클러스터를 구축할 수 있다는 의미).



#### 대규모 모델 배포

Spark를 사용하여 많은 양의 데이터에 훈련된 모델을 만들 수 있다.



### SparkFlow : Train Tensorflow Models with Apache Spark Pipelines

교수님께서 주신 링크 2번 내용.

Spark와 SparkML은 대규모 데이터 세트에서 복잡한 파이프라인을 조율할 수 있는 아키텍쳐를 지원한다.
하지만 현재 이미지용 CNN이나 자연어 처리를 위한 RNN등은 지원하지 않는다.

feature engineering pipeline은 원시데이터를 정리하고 학스모델로 변환 및 준비하는 프로세스를 크게 단순화시킨다.

Tensorflow는 다양한 신경망을 학습시키는데 적합하지만 Spark에 비해 대형 데이터셋의 파이프라인에 대한 기능지원이 부족하다. 

Spark에서 Tensorflow 모델을 교육하기 위해 다른 오픈소스 라이브러리가 존재하지만, SparkML의 기능을 모두 이용하는 개발자는 거의 없다.

이에따라 2018에 LifeOmic에서 Sparkflow를 출시했다. Spark의 파이프라인 API 인터페이스를 Tensorflow와 결합한다. pip을 통해 설치가 가능하다.  



## 스파크의 기본 아키텍쳐







## 참고자료

- Spark : The Definitive Guide , 스파크 완벽 가이드 , 한빛미디어
- http://www.itworld.co.kr/news/107192
- https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html - 교수님께서 주신 링크 1 (2016.01.25)
- https://medium.com/lifeomic/sparkflow-train-tensorflow-models-with-apache-spark-pipelines-74dca32f60f3 - - 교수님께서 주신 링크 2 (2018.08.24)
- https://github.com/lifeomic/sparkflow - 교수님께서 주신 링크 2에서 들어감
- [http://www.engear.net/wp/hyper-paramertesr-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/](http://www.engear.net/wp/hyper-paramertesr-머신러닝/) - 하이퍼파라미터
- https://databuzz-team.github.io/2018/12/05/hyperparameter-setting/ - 하이퍼파라미터 튜닝이란
- http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html - 베이지안 최적화 1
- https://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html - 베이지안 최적화 2




