{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 : 데이터프레임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터프레임은 관계형 데이터베이스의 테이블에서 칼럼 이름으로 구성된 변경 불가능한 분산 데이터 컬렉션이다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our own JSON data \n",
    "# This way we don't have to access the file system yet.\n",
    "# RDD를 생성하는 코드\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD를 생성했으니, 이것을 spark.read.json 함수를 사용해서 RDD로 바꿀 것이다.\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query \n",
    "# 모든 행이 출력되는 sql구문이다\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD로 연동하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있는 RDD를 데이터프레임(또는 Dataset[T])으로 변경하는 두 가지 방법이 있다.\n",
    "\n",
    "1. reflection을 사용해 스키마을 추측: 더욱 자세한 코드를 작성하게 할 수 있다.\n",
    "\n",
    "2. 스키마를 직접적으로 코드에 명시: 열과 데이터 타입이 런타임에 드러날 때 데이터프레임의 구조를 만들도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reflection을 이용한 스키마 추측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터프레임을 빌드하고 쿼리를 수행하는 과정에서 데이터프레임에 대한 스키마는 자동으로 정의된다. 최초에 행 객체는 key-value 리스트가 행 클래스의 **kwargs로 전달되면서 구성된다. 그 후, 스파크 SQL은 이 행 객체의 RDD를 데이터프레임으로 변경한다. 키는 column이고 데이터 타입은 데이터 샘플링을 통해 추측된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 id가 string이 아니라 long타입이기를 의도한 것이라서 스키마를 명시하고 싶다면 스키마를 프로그래밍하듯이 명시해야 한다\n",
    "\n",
    "[참고사항]\n",
    "\n",
    "아래 코드에서 id로 들어가는 데이터들에 따옴표를 씌운다음 long으로 명시하면 스키마 자체는 명시한대로 long이 되지만 실제 DataFrame으로 만들 때 '123'이 long 타입으로 변환되지 않아서 에러가 난다.\n",
    "\n",
    "다시말해 string형식으로 적고 스키마를 long으로 했을 때 강제형변환이 되지는 않는다는 것이다. 심심하면 123,234,345에 따옴표를 씌워보자. 밑에서 swimmers.count()를 할 때 런타임에러가 날 것이다. \n",
    "\n",
    "에러가 저 때 나는 이유는 스파크특성상 count()라는 액션을 취하기 전까지 데이터가 실제로 만들어지지 않기 때문이다. 이 말이 이해가 되지 않는다면 1장 2장을 다시 복습할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "\n",
    "\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "# Notice that we have redefined id as Long (instead of String)\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(swimmers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임 API로 Query하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect(), show(), take() 함수를 이용해 데이터프레임 내의 데이터를 볼 수 있다. show()와 take() 함수는 리턴되는 행의 개수를 제한하는 옵션을 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the values \n",
    "swimmers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Databricks `display` command to view the data easier\n",
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter문 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이가 22살인 행에서 id와 age추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "눈 색깔이 b로 시작하는 수영선수의 이름 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the name, eyeColor where eyeColor like 'b%'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL로 쿼리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터프레임 API로 진행했던 연산을 SQL로 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL Query and return the data\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of rows in SQL\n",
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임 시나리오 : 비행 기록 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set File Paths\n",
    "flightPerfFilePath = \"flight-data/departuredelays.csv\"\n",
    "airportsFilePath = \"flight-data/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset \n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 도시와 출발지 코드에 다라 비행 지연 합을 쿼리하기 (워싱턴 주에 대해)\n",
    "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|State|   Delays|\n",
      "+-----+---------+\n",
      "|   SC|  80666.0|\n",
      "|   AZ| 401793.0|\n",
      "|   LA| 199136.0|\n",
      "|   MN| 256811.0|\n",
      "|   NJ| 452791.0|\n",
      "|   OR| 109333.0|\n",
      "|   VA|  98016.0|\n",
      "| null| 397237.0|\n",
      "|   RI|  30760.0|\n",
      "|   WY|  15365.0|\n",
      "|   KY|  61156.0|\n",
      "|   NH|  20474.0|\n",
      "|   MI| 366486.0|\n",
      "|   NV| 474208.0|\n",
      "|   WI| 152311.0|\n",
      "|   ID|  22932.0|\n",
      "|   CA|1891919.0|\n",
      "|   CT|  54662.0|\n",
      "|   NE|  59376.0|\n",
      "|   MT|  19271.0|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모든 주에 대해 계속 진행하기\n",
    "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(State='SC', Delays=80666.0),\n",
       " Row(State='AZ', Delays=401793.0),\n",
       " Row(State='LA', Delays=199136.0),\n",
       " Row(State='MN', Delays=256811.0),\n",
       " Row(State='NJ', Delays=452791.0),\n",
       " Row(State='OR', Delays=109333.0),\n",
       " Row(State='VA', Delays=98016.0),\n",
       " Row(State=None, Delays=397237.0),\n",
       " Row(State='RI', Delays=30760.0),\n",
       " Row(State='WY', Delays=15365.0),\n",
       " Row(State='KY', Delays=61156.0),\n",
       " Row(State='NH', Delays=20474.0),\n",
       " Row(State='MI', Delays=366486.0),\n",
       " Row(State='NV', Delays=474208.0),\n",
       " Row(State='WI', Delays=152311.0),\n",
       " Row(State='ID', Delays=22932.0),\n",
       " Row(State='CA', Delays=1891919.0),\n",
       " Row(State='CT', Delays=54662.0),\n",
       " Row(State='NE', Delays=59376.0),\n",
       " Row(State='MT', Delays=19271.0),\n",
       " Row(State='NC', Delays=394256.0),\n",
       " Row(State='VT', Delays=14755.0),\n",
       " Row(State='MD', Delays=362845.0),\n",
       " Row(State='MO', Delays=344538.0),\n",
       " Row(State='IL', Delays=1630792.0),\n",
       " Row(State='ME', Delays=15214.0),\n",
       " Row(State='ND', Delays=27402.0),\n",
       " Row(State='WA', Delays=172439.0),\n",
       " Row(State='MS', Delays=33827.0),\n",
       " Row(State='AL', Delays=86308.0),\n",
       " Row(State='IN', Delays=122708.0),\n",
       " Row(State='OH', Delays=325897.0),\n",
       " Row(State='TN', Delays=291015.0),\n",
       " Row(State='NM', Delays=64422.0),\n",
       " Row(State='IA', Delays=65128.0),\n",
       " Row(State='PA', Delays=324270.0),\n",
       " Row(State='SD', Delays=28790.0),\n",
       " Row(State='NY', Delays=878929.0),\n",
       " Row(State='TX', Delays=1994943.0),\n",
       " Row(State='WV', Delays=8408.0),\n",
       " Row(State='GA', Delays=1191014.0),\n",
       " Row(State='MA', Delays=238602.0),\n",
       " Row(State='KS', Delays=23752.0),\n",
       " Row(State='CO', Delays=963061.0),\n",
       " Row(State='FL', Delays=1531877.0),\n",
       " Row(State='AK', Delays=5384.0),\n",
       " Row(State='AR', Delays=69453.0),\n",
       " Row(State='OK', Delays=109114.0),\n",
       " Row(State='UT', Delays=162395.0),\n",
       " Row(State='HI', Delays=36825.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리 결과에 대한 모든 결과 보기\n",
    "queryResult = spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State\").collect()\n",
    "queryResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
