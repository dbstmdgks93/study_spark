{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 생성하기\n",
    "pyspark에서 RDD를 생성하는 방법은 두 가지이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 collection에 대해 parallelize() 함수를 수행하거나, 외부 어딘가 또는 특정 위치에 저장된 파일을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file = sc.textFile(\"D://Pyspark_dataset/VS14MORT.txt.gz\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 다운로드 : http://tomdrabas.com/data/VS14MORT.txt.gz \n",
    "\n",
    "sc.textFile(...,n) 의 마지막 매개변수 n은 데이터셋이 나눠진 파티션의 개수를 의미한다. 책의 저자에 따르면 한 클러스터에서는 2~4개의 파티션으로 데이터셋을 나누는 것이 가장 좋다고 한다.\n",
    "\n",
    "스파크는 NTFS, FAT, Mac OS Extended 또는 HDFS, S3, 카산드라 등의 분산파일시스템과 같은 여러 개의 파일시스템으로부터 데이터를 읽을 수 있다.\n",
    "\n",
    "여러 가지 데이터 포맷이 지원된다. 텍스트, 파케이, JSON, 하이브테이블 그리고 관계형 데이터베이스에서의 데이터는 JDBC 드라이버로 읽을 수 있다. 스파크는 압축된 데이터셋도 읽어서 작업할 수 있다.\n",
    "\n",
    "데이터가 어떻게 읽히느냐에 따라서, 데이터를 지니고 있는 객체는 조금씩 다르게 표현된다. 데이터가 읽히는 파일은 MapPartitionsRDD로 표현된다. 컬렉션에 대해 .parallelize() 함수를 돌릴 때처럼 ParallelCollectionRDD를 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame과 달리 RDD는 schema-less 데이터 구조이다. 그러므로 RDD를 사용할 때 데이터셋을 병렬 처리하는 것은 전혀 문제가 되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain', 'visited', 4504]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous = sc.parallelize([('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain','visited', 4504]]).collect()\n",
    "data_heterogenous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 tuple, dict, list 와 같은 거의 모든 데이터 구조를 섞을 수 있고, 이는 스파크에서 전혀 문제가 되지 않는다. 데이터셋에 대해 .collect() 함수를 수행하면(즉, 데이터셋을 드라이버에 다시 가져오기 위해 액션을 수행하면) 파이썬에서 일반적으로 했던 방식대로 객체 내의 데이터에 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous[1]['Porsche']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".collect() 함수는 RDD의 모든 엘리먼트를 드라이버에 리턴하고, 드라이버에서 엘리먼트들은 리스트로 나열된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일로부터 데이터 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일로부터 데이터를 읽을 때, 파일의 각 행이 RDD의 한 엘리먼트를 이룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                   1                                          2101  M1087 432311  4M4                2014U7CN                                    I64 238 070   24 0111I64                                                                                                                                                                           01 I64                                                                                                  01  11                                 100 601']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 람다 표현\n",
    "책에서는 위의 표현이 보기 어렵다며 아래의 함수를 정의했다. 하지만 결과를 보면 그것도 딱히 보기 좋은 모양은 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [\n",
    "         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n",
    "         19,21,22,23,24,25,27,28,29,30,32,33,34,\n",
    "         36,37,38,39,40,41,42,43,44,45,46,47,48,\n",
    "         49,50,51,52,53,54,55,56,58,60,61,62,63,\n",
    "         64,65,66,67,68,69,70,71,72,73,74,75,76,\n",
    "         77,78,79,81,82,83,84,85,87,89\n",
    "    ]\n",
    "\n",
    "    '''\n",
    "        Input record schema\n",
    "        schema: n-m (o) -- xxx\n",
    "            n - position from\n",
    "            m - position to\n",
    "            o - number of characters\n",
    "            xxx - description\n",
    "        1. 1-19 (19) -- reserved positions\n",
    "        2. 20 (1) -- resident status\n",
    "        3. 21-60 (40) -- reserved positions\n",
    "        4. 61-62 (2) -- education code (1989 revision)\n",
    "        5. 63 (1) -- education code (2003 revision)\n",
    "        6. 64 (1) -- education reporting flag\n",
    "        7. 65-66 (2) -- month of death\n",
    "        8. 67-68 (2) -- reserved positions\n",
    "        9. 69 (1) -- sex\n",
    "        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated\n",
    "        11. 71-73 (3) -- number of units (years, months etc)\n",
    "        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)\n",
    "        13. 75-76 (2) -- age recoded into 52 categories\n",
    "        14. 77-78 (2) -- age recoded into 27 categories\n",
    "        15. 79-80 (2) -- age recoded into 12 categories\n",
    "        16. 81-82 (2) -- infant age recoded into 22 categories\n",
    "        17. 83 (1) -- place of death\n",
    "        18. 84 (1) -- marital status\n",
    "        19. 85 (1) -- day of the week of death\n",
    "        20. 86-101 (16) -- reserved positions\n",
    "        21. 102-105 (4) -- current year\n",
    "        22. 106 (1) -- injury at work\n",
    "        23. 107 (1) -- manner of death\n",
    "        24. 108 (1) -- manner of disposition\n",
    "        25. 109 (1) -- autopsy\n",
    "        26. 110-143 (34) -- reserved positions\n",
    "        27. 144 (1) -- activity code\n",
    "        28. 145 (1) -- place of injury\n",
    "        29. 146-149 (4) -- ICD code\n",
    "        30. 150-152 (3) -- 358 cause recode\n",
    "        31. 153 (1) -- reserved position\n",
    "        32. 154-156 (3) -- 113 cause recode\n",
    "        33. 157-159 (3) -- 130 infant cause recode\n",
    "        34. 160-161 (2) -- 39 cause recode\n",
    "        35. 162 (1) -- reserved position\n",
    "        36. 163-164 (2) -- number of entity-axis conditions\n",
    "        37-56. 165-304 (140) -- list of up to 20 conditions\n",
    "        57. 305-340 (36) -- reserved positions\n",
    "        58. 341-342 (2) -- number of record axis conditions\n",
    "        59. 343 (1) -- reserved position\n",
    "        60-79. 344-443 (100) -- record axis conditions\n",
    "        80. 444 (1) -- reserve position\n",
    "        81. 445-446 (2) -- race\n",
    "        82. 447 (1) -- bridged race flag\n",
    "        83. 448 (1) -- race imputation flag\n",
    "        84. 449 (1) -- race recode (3 categories)\n",
    "        85. 450 (1) -- race recode (5 categories)\n",
    "        86. 461-483 (33) -- reserved positions\n",
    "        87. 484-486 (3) -- Hispanic origin\n",
    "        88. 487 (1) -- reserved\n",
    "        89. 488 (1) -- Hispanic origin/race recode\n",
    "     '''\n",
    "\n",
    "    record_split = re\\\n",
    "        .compile(\n",
    "            r'([\\s]{19})([0-9]{1})([\\s]{40})([0-9\\s]{2})([0-9\\s]{1})([0-9]{1})([0-9]{2})' + \n",
    "            r'([\\s]{2})([FM]{1})([0-9]{1})([0-9]{3})([0-9\\s]{1})([0-9]{2})([0-9]{2})' + \n",
    "            r'([0-9]{2})([0-9\\s]{2})([0-9]{1})([SMWDU]{1})([0-9]{1})([\\s]{16})([0-9]{4})' +\n",
    "            r'([YNU]{1})([0-9\\s]{1})([BCOU]{1})([YNU]{1})([\\s]{34})([0-9\\s]{1})([0-9\\s]{1})' +\n",
    "            r'([A-Z0-9\\s]{4})([0-9]{3})([\\s]{1})([0-9\\s]{3})([0-9\\s]{3})([0-9\\s]{2})([\\s]{1})' + \n",
    "            r'([0-9\\s]{2})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([\\s]{36})([A-Z0-9\\s]{2})([\\s]{1})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([\\s]{1})([0-9\\s]{2})([0-9\\s]{1})' + \n",
    "            r'([0-9\\s]{1})([0-9\\s]{1})([0-9\\s]{1})([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n",
    "    try:\n",
    "        rs = np.array(record_split.split(row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs\n",
    "#     return record_split.split(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['1', '  ', '2', '1', '01', 'M', '1', '087', ' ', '43', '23', '11',\n",
       "        '  ', '4', 'M', '4', '2014', 'U', '7', 'C', 'N', ' ', ' ', 'I64 ',\n",
       "        '238', '070', '   ', '24', '01', '11I64  ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '01',\n",
       "        'I64  ', '     ', '     ', '     ', '     ', '     ', '     ',\n",
       "        '     ', '     ', '     ', '     ', '     ', '     ', '     ',\n",
       "        '     ', '     ', '     ', '     ', '     ', '     ', '01', ' ',\n",
       "        ' ', '1', '1', '100', '6'], dtype='<U40')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file_conv = data_from_file.map(extractInformation)\n",
    "data_from_file_conv.map(lambda row: row).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전역범위 vs 지역범위\n",
    "\n",
    "Pyspark 사용자로서 익숙해져야 할 것 중 하나는 스파크의 병렬처리다. 파이썬에 능숙하더라도 Pyspark에서 스크립트를 실행하는 것에 어려움을 느낄 수 있다.\n",
    "\n",
    "스파크는 로컬 모드와 클러스터 모드로 동작할 수 있다. 스파크가 로컬 모드로 동작할 때는 파이썬을 실행시키는 것과 다르지 않을 수도 있다. 하지만 특별한 주의 없이 클러스터모드에서 같은 코드를 실행하면 골치아픈일이 많이 생긴다. \n",
    "\n",
    "클러스터 모드에서 job이 실행되면, 그 job은 마스터 노드에 보내진다. 마스터노드는 job을 위해 DAG을 생성하고 어떤 워커 노드가 특정 태스크를 실행할지 결정한다. 그리고 드라이버는 각 태스크를 마칠 준비를 한다. 워커 노드가 태스크를 수행하고 작업을 마치면 그 결과를 마스터 노드에 리턴하도록 한다. \n",
    "\n",
    "이 변수와 함수는 내부적으로 실행 노드의 문맥 상에서 정적이다. 즉, 각자의 실행 노드가 드라이버 노드에서 사용되는 변수와 함수를 복사해 사용한다. 태스크를 실행할 때, 실행 노드가 이 변수나 함수를 수정할 경우 다른 실행 노드들의 변수나 함수에는 영향을 주지 않는다. 이로 인해 런타임 버그나 몇몇 이상 행위를 유발할 수 있는데, 보통 이러한 오류들은 굉장히 추적하기가 어렵다. \n",
    "\n",
    "추가적인 예제는 http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VS14MORT.txt.gz MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
