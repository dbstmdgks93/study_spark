# 스파크 프로그램의 실행 과정

스파크를 하둡의 HDFS 에서 실행시킨다고 가정하고 설명한다.
300MB 크기의 로그 파일이 노드 3개로 구성된 HDFS 클러스터에 분산 저장되어 있다고 하자. HDFS는 이 파일을 자동으로 128MB 크기의 청크(chunk) 로 분할하고(하둡에서는 block 이라는 용어를 사용한다), 각 블록을 클러스터의 여러 노드에 나누어 저장한다. 